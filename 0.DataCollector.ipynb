{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader \n",
    "This notebook loading images from ISIC Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if this notebook called from main one\n",
    "try: IS_MAIN\n",
    "except: IS_MAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader mode: STANDALONE\n"
     ]
    }
   ],
   "source": [
    "# setup necessary parameters\n",
    "if IS_MAIN:\n",
    "    print('DataLoader mode: MAIN')\n",
    "    limit = 600\n",
    "    offset = 0\n",
    "    \n",
    "else:\n",
    "    print('DataLoader mode: STANDALONE')\n",
    "    limit = 500\n",
    "    offset = 0\n",
    "\n",
    "threads_timeout = 60\n",
    "\n",
    "# Spesify output folders\n",
    "meta_dir = 'data/ISIC'\n",
    "img_dir = meta_dir + '/images'\n",
    "mask_dir = meta_dir + '/masks'\n",
    "img_info_dir = meta_dir + '/info_images'\n",
    "mask_info_dir = meta_dir + '/info_masks'\n",
    "images_info_fn = 'images_info'\n",
    "masks_info_fn = 'masks_info'\n",
    "\n",
    "if not os.path.exists(meta_dir): os.makedirs(meta_dir)\n",
    "if not os.path.exists(img_dir): os.makedirs(img_dir)\n",
    "if not os.path.exists(mask_dir): os.makedirs(mask_dir)\n",
    "if not os.path.exists(img_info_dir): os.makedirs(img_info_dir)\n",
    "if not os.path.exists(mask_info_dir): os.makedirs(mask_info_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make ISIC API requester "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make ISIC Api request class\n",
    "'''\n",
    "import requests\n",
    "\n",
    "class ISICApi(object):\n",
    "    def __init__(self, hostname='https://isic-archive.com',\n",
    "                 username=None, password=None):\n",
    "        self.baseUrl = f'{hostname}/api/v1'\n",
    "        self.authToken = None\n",
    "\n",
    "        if username is not None:\n",
    "            if password is None:\n",
    "                password = input(f'Password for user \"{username}\":')\n",
    "            self.authToken = self._login(username, password)\n",
    "\n",
    "    def _makeUrl(self, endpoint):\n",
    "        return f'{self.baseUrl}/{endpoint}'\n",
    "\n",
    "    def _login(self, username, password):\n",
    "        authResponse = requests.get(\n",
    "            self._makeUrl('user/authentication'),\n",
    "            auth=(username, password)\n",
    "        )\n",
    "        if not authResponse.ok:\n",
    "            raise Exception(f'Login error: {authResponse.json()[\"message\"]}')\n",
    "\n",
    "        authToken = authResponse.json()['authToken']['token']\n",
    "        return authToken\n",
    "\n",
    "    def get(self, endpoint):\n",
    "        url = self._makeUrl(endpoint)\n",
    "        headers = {'Girder-Token': self.authToken} if self.authToken else None\n",
    "        return requests.get(url, headers=headers)\n",
    "\n",
    "    def getJson(self, endpoint):\n",
    "        return self.get(endpoint).json()\n",
    "\n",
    "    def getJsonList(self, endpoint):\n",
    "        endpoint += '&' if '?' in endpoint else '?'\n",
    "        LIMIT = 50\n",
    "        offset = 0\n",
    "        while True:\n",
    "            resp = self.get(\n",
    "                f'{endpoint}limit={LIMIT:d}&offset={offset:d}'\n",
    "            ).json()\n",
    "            if not resp:\n",
    "                break\n",
    "            for elem in resp:\n",
    "                yield elem\n",
    "            offset += LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of images names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 500 images names.\n"
     ]
    }
   ],
   "source": [
    "#Insert Username and Password Below\n",
    "api = ISICApi(username=\"veaxvoid\", password=\"ZXCASD123qweasd\")\n",
    "\n",
    "image_list = api.getJson('image?limit={}&offset={}&sort=name'.format(limit, offset))\n",
    "\n",
    "print('Requested {} images names.'.format(len(image_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded information about 497 images.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load info for each img\n",
    "'''\n",
    "def load_img_info(img_id, img_name):\n",
    "    image_info = api.getJson('image/{}'.format(img_id))\n",
    "    \n",
    "#     if image_info['dataset']['name'] != 'SONIC':\n",
    "    file_path = os.path.join(img_info_dir, img_name+'.json')\n",
    "    file = open(file_path, \"w\")\n",
    "    json.dump(image_info, file)\n",
    "    file.close()\n",
    "    \n",
    "imgs_id = [info['_id'] for info in image_list]\n",
    "imgs_name = [info['name'] for info in image_list]\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(load_img_info, imgs_id, imgs_name, timeout=threads_timeout)\n",
    "\n",
    "print('Loaded information about {} images.'.format(len(os.listdir(img_info_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images: 497.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Collect all image info from json files in to csv table\n",
    "'''\n",
    "# read filenames in infoimages dir\n",
    "infoimgs_filenames = sorted([f for f in os.listdir(img_info_dir)])\n",
    "\n",
    "image_details = []\n",
    "\n",
    "for img_name in infoimgs_filenames:\n",
    "    file_path = os.path.join(img_info_dir, img_name)\n",
    "    file = open(file_path, 'r')\n",
    "    info = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    image_details += [info]\n",
    "\n",
    "print('Number of valid images: {}.'.format(len(image_details)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata to CSV: images_info.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Image info to csv\n",
    "'''\n",
    "file_path = os.path.join(meta_dir, images_info_fn+'.csv')\n",
    "\n",
    "# Determine the union of all image metadata fields\n",
    "metadata_fields = set(\n",
    "    field\n",
    "    for image_detail in image_details\n",
    "    for field in image_detail['meta']['clinical'].keys()\n",
    ")\n",
    "\n",
    "metadata_fields = ['isic_name','isic_id'] + sorted(metadata_fields) + ['dataset_name']\n",
    "\n",
    "# Write the metadata to a CSV\n",
    "with open(file_path, 'w') as outputStream:\n",
    "    csvWriter = csv.DictWriter(outputStream, metadata_fields)\n",
    "    csvWriter.writeheader()\n",
    "    \n",
    "    for image_detail in image_details:\n",
    "        row_dict = image_detail['meta']['clinical'].copy()\n",
    "        row_dict['isic_name'] = image_detail['name']\n",
    "        row_dict['isic_id'] = image_detail['_id']\n",
    "        row_dict['dataset_name'] = image_detail['dataset']['name']\n",
    "        csvWriter.writerow(row_dict)\n",
    "\n",
    "print('Writing metadata to CSV: {}'.format(images_info_fn+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        isic_name                   isic_id  age_approx anatom_site_general benign_malignant diagnosis diagnosis_confirm_type melanocytic  sex dataset_name\n",
      "492  ISIC_0000495  5436e3f4bae478396759f4b2         NaN     lower extremity           benign     nevus                    NaN        True  NaN        UDA-1\n",
      "493  ISIC_0000496  5436e3f4bae478396759f4b4         NaN     posterior torso           benign     nevus                    NaN        True  NaN        UDA-1\n",
      "494  ISIC_0000497  5436e3f5bae478396759f4b6         NaN     posterior torso           benign     nevus                    NaN        True  NaN        UDA-1\n",
      "495  ISIC_0000498  5436e3f5bae478396759f4b8         NaN      anterior torso           benign     nevus                    NaN        True  NaN        UDA-1\n",
      "496  ISIC_0000499  5436e3f5bae478396759f4ba         NaN       lateral torso           benign     nevus                    NaN        True  NaN        UDA-1\n",
      "Data summary:\n",
      "{'nevus': 358, 'melanoma': 137, nan: 0}\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(meta_dir, images_info_fn+'.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "images_ids = list(data['isic_id'])\n",
    "images_names = list(data['isic_name'])\n",
    "\n",
    "if not IS_MAIN:\n",
    "    print(data.tail().to_string())\n",
    "    info_sum = {d:(data['diagnosis'] == d).sum() for d in data['diagnosis'].unique()}\n",
    "    print('Data summary:')\n",
    "    print(info_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 495 images.\n"
     ]
    }
   ],
   "source": [
    "def load_imgs(img_id, img_name):\n",
    "    img_file = api.get('image/{}/download'.format(img_id))\n",
    "    img_file.raise_for_status()\n",
    "    file_path = os.path.join(img_dir, '{}.png'.format(img_name))\n",
    "  \n",
    "    with open(file_path, 'wb') as out_stream:\n",
    "        for chunk in img_file:\n",
    "            out_stream.write(chunk)\n",
    "    \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(load_imgs, images_ids, images_names, timeout=threads_timeout)\n",
    "\n",
    "print('Load {} images.'.format(len(os.listdir(img_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reload in folder data/ISIC/images: 497 files.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "if some files don't loaded. Some threads are dying and dont load files\n",
    "'''\n",
    "def reload(curr_dir, all_names, prefix, loader, images_names=images_names, images_ids=images_ids):\n",
    "    \n",
    "    curr_files = np.array(sorted(os.listdir(curr_dir)))\n",
    "    all_files = np.array([name+prefix for name in all_names])\n",
    "\n",
    "    result_names = []\n",
    "    result_ids = []\n",
    "\n",
    "    for i in range(len(all_files)): \n",
    "        if not np.isin(all_files[i], curr_files, assume_unique=True):\n",
    "            result_names += [images_names[i]]\n",
    "            result_ids += [images_ids[i]]\n",
    "\n",
    "    if len(result_names) != 0:\n",
    "        with ThreadPoolExecutor() as e: e.map(loader, result_ids, result_names, timeout=threads_timeout)\n",
    "\n",
    "    print('After reload in folder {}: {} files.'.format(curr_dir,len(os.listdir(curr_dir))))\n",
    "    \n",
    "reload(img_dir, images_names, '.png', load_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load segmentation masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 494 info about masks.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load segmentation data\n",
    "'''\n",
    "def load_mask_info(img_id, img_name):\n",
    "    segmentation_data = api.getJson('segmentation?imageId={}'.format(img_id))\n",
    "    \n",
    "    file_path = os.path.join(mask_info_dir, img_name+'.json')\n",
    "    file = open(file_path, \"w\")\n",
    "    json.dump(segmentation_data, file)\n",
    "    file.close()\n",
    "    \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(load_mask_info, images_ids, images_names, timeout=threads_timeout)\n",
    "\n",
    "print('Load {} info about masks.'.format(len(os.listdir(mask_info_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reload in folder data/ISIC/info_masks: 497 files.\n"
     ]
    }
   ],
   "source": [
    "reload(mask_info_dir, images_names, '.json', load_mask_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segmentation info files: 497\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Collect all VALID masks info from json files \n",
    "'''\n",
    "segmentation_details = []\n",
    "\n",
    "for img_name in images_names:\n",
    "    file_path = os.path.join(mask_info_dir, img_name+'.json')\n",
    "    file = open(file_path, 'r')\n",
    "    info = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    if info:\n",
    "        if not info[0]['failed']:\n",
    "            segmentation_details += [info[0]]\n",
    "            segmentation_details[-1]['name'] = img_name\n",
    "\n",
    "print('Number of segmentation info files: {}'.format(len(segmentation_details)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata to CSV: masks_info.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Masks info to csv\n",
    "'''\n",
    "file_path = os.path.join(meta_dir, masks_info_fn+'.csv')\n",
    "\n",
    "# Determine the union of all image metadata fields\n",
    "metadata_fields = set(\n",
    "    segmentation_details[0].keys()\n",
    ")\n",
    "\n",
    "metadata_fields = sorted(metadata_fields)\n",
    "\n",
    "# Write the metadata to a CSV\n",
    "with open(file_path, 'w') as outputStream:\n",
    "    csvWriter = csv.DictWriter(outputStream, metadata_fields)\n",
    "    csvWriter.writeheader()\n",
    "    \n",
    "    k=-1\n",
    "    for info in segmentation_details:\n",
    "        k+=1\n",
    "        row_dict = info.copy()\n",
    "        csvWriter.writerow(row_dict)\n",
    "\n",
    "print('Writing metadata to CSV: {}'.format(masks_info_fn+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          _id                           created  failed          name   skill\n",
      "492  54cc02c6bae47819d8e4c9a8  2015-01-30T22:16:38.471000+00:00   False  ISIC_0000495  expert\n",
      "493  544eae26bae478661558fb7e  2014-10-27T20:42:14.795000+00:00   False  ISIC_0000496  expert\n",
      "494  54cc029fbae47819d8e4c99c  2015-01-30T22:15:59.142000+00:00   False  ISIC_0000497  expert\n",
      "495  54cc0254bae47819d8e4c996  2015-01-30T22:14:44.642000+00:00   False  ISIC_0000498  expert\n",
      "496  57c04f259fc3c158f2bd0e3b  2016-08-26T14:16:05.388000+00:00   False  ISIC_0000499  expert\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "\n",
    "masks_names = list(data['name'])\n",
    "masks_id = list(data['_id'])\n",
    "\n",
    "if not IS_MAIN:\n",
    "    print(data.tail().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 491 info about masks.\n"
     ]
    }
   ],
   "source": [
    "def load_mask(mask_id, img_name):\n",
    "    img_file = api.get('segmentation/{}/mask'.format(mask_id))\n",
    "    img_file.raise_for_status()\n",
    "    file_path = os.path.join(mask_dir, '{}_mask.png'.format(img_name))\n",
    "\n",
    "    with open(file_path, 'wb') as out_stream:\n",
    "        for chunk in img_file:\n",
    "            out_stream.write(chunk)    \n",
    "            \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(load_mask, masks_id, masks_names, timeout=threads_timeout)\n",
    "\n",
    "print('Load {} info about masks.'.format(len(os.listdir(mask_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reload in folder data/ISIC/masks: 497 files.\n"
     ]
    }
   ],
   "source": [
    "reload(mask_dir, masks_names, '_mask.png', load_mask, masks_names, masks_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
