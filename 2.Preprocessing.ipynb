{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We want to generate transformed images before training  \n",
    "\n",
    "**No Augmentation**\n",
    "- [x] Resize\n",
    "- [x] ToTensor\n",
    "\n",
    "**Basic Set**\n",
    "- [x] B - Saturation, Contrast, Brightness\n",
    "- [x] C - Saturation, Contrast, Brightness, and Hue\n",
    "- [x] D - Affine\n",
    "- [ ] E - Flips\n",
    "- [ ] F - Random Crops\n",
    "- [ ] G - Random Erasing\n",
    "- [ ] H - Elastic deformation\n",
    "- [ ] I - Lesion Mix (mix of 2 images)  \n",
    "\n",
    "**Advance set**\n",
    "- [ ] J - Basic Set F → D → E → C.\n",
    "- [ ] K - Basic Set + Erasing F → G → D → E → C.\n",
    "- [ ] L - Basic Set + Elastic F → D → H → E → C.\n",
    "- [ ] M - Basic Set + Mix I → F → D → E → C.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if this notebook called from main one\n",
    "try: IS_MAIN\n",
    "except: IS_MAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing mode: STANDALONE\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "\n",
    "# setup necessary parameters\n",
    "if IS_MAIN:\n",
    "    print('Preprocessing mode: MAIN')\n",
    "    \n",
    "else:\n",
    "    print('Preprocessing mode: STANDALONE')\n",
    "    \n",
    "    # Transformation config\n",
    "    t_config = {'seg':{\n",
    "        'train_size':500,\n",
    "        'val_size':200,\n",
    "        'test_size':100\n",
    "                    },\n",
    "              'clf':{\n",
    "        'train_size':{'nev':250,'mel':250},\n",
    "        'val_size':{'nev':100,'mel':100},\n",
    "        'test_size':{'nev':50,'mel':50}\n",
    "                    },\n",
    "              'img_size':(64,64)\n",
    "             }\n",
    "    \n",
    "    # define input folders\n",
    "    base_dir = 'data/raw'\n",
    "    seg_dir = join(base_dir, 'segmentation')\n",
    "    clf_dir = join(base_dir, 'classification')\n",
    "\n",
    "    # Segmentation\n",
    "    seg_validation_dir = join(seg_dir, 'validation')\n",
    "    seg_train_dir = join(seg_dir, 'train')\n",
    "    seg_test_dir = join(seg_dir, 'test')\n",
    "\n",
    "    seg_validation_img_dir = join(seg_validation_dir, 'images')\n",
    "    seg_validation_msk_dir = join(seg_validation_dir, 'masks')\n",
    "    seg_train_img_dir = join(seg_train_dir, 'images')\n",
    "    seg_train_msk_dir = join(seg_train_dir, 'masks')\n",
    "    seg_test_img_dir = join(seg_test_dir, 'images')\n",
    "    seg_test_msk_dir = join(seg_test_dir, 'masks')\n",
    "\n",
    "    # Classification\n",
    "    clf_validation_dir = join(clf_dir, 'validation')\n",
    "    clf_train_dir = join(clf_dir, 'train')\n",
    "    clf_test_dir = join(clf_dir, 'test')\n",
    "\n",
    "    clf_validation_nev_dir = join(clf_validation_dir, 'nevus')\n",
    "    clf_validation_mel_dir = join(clf_validation_dir, 'melanoma')\n",
    "    clf_train_nev_dir = join(clf_train_dir, 'nevus')\n",
    "    clf_train_mel_dir = join(clf_train_dir, 'melanoma')\n",
    "    clf_test_nev_dir = join(clf_test_dir, 'nevus')\n",
    "    clf_test_mel_dir = join(clf_test_dir, 'melanoma')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folders for transfored files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transform_dirs():\n",
    "    t = {}\n",
    "    t['base'] = 'data/transforms'\n",
    "    t['seg'] = {}\n",
    "    t['clf'] = {}\n",
    "    t['seg']['base'] = join(t['base'], 'segmentation')\n",
    "    t['clf']['base'] = join(t['base'], 'classification')\n",
    "\n",
    "    t['seg']['val'] = {}\n",
    "    t['seg']['train'] = {}\n",
    "    t['seg']['test'] = {}\n",
    "    t['seg']['val']['base'] = join(t['seg']['base'], 'validation')\n",
    "    t['seg']['train']['base'] = join(t['seg']['base'], 'train')\n",
    "    t['seg']['test']['base'] = join(t['seg']['base'], 'test')\n",
    "\n",
    "    t['seg']['val']['img'] = join(t['seg']['val']['base'], 'images')\n",
    "    t['seg']['val']['msk'] = join(t['seg']['val']['base'], 'masks')\n",
    "    t['seg']['train']['img'] = join(t['seg']['train']['base'], 'images')\n",
    "    t['seg']['train']['msk'] = join(t['seg']['train']['base'], 'masks')\n",
    "    t['seg']['test']['img'] = join(t['seg']['test']['base'], 'images')\n",
    "    t['seg']['test']['msk'] = join(t['seg']['test']['base'], 'masks')\n",
    "\n",
    "    t['clf']['val'] = {}\n",
    "    t['clf']['train'] = {}\n",
    "    t['clf']['test'] = {}\n",
    "    t['clf']['val']['base'] = join(t['clf']['base'], 'validation')\n",
    "    t['clf']['train']['base'] = join(t['clf']['base'], 'train')\n",
    "    t['clf']['test']['base'] = join(t['clf']['base'], 'test')\n",
    "\n",
    "    t['clf']['val']['nev'] = join(t['clf']['val']['base'], 'nevus')\n",
    "    t['clf']['val']['mel'] = join(t['clf']['val']['base'], 'melanoma')\n",
    "    t['clf']['train']['nev'] = join(t['clf']['train']['base'], 'nevus')\n",
    "    t['clf']['train']['mel'] = join(t['clf']['train']['base'], 'melanoma')\n",
    "    t['clf']['test']['nev'] = join(t['clf']['test']['base'], 'nevus')\n",
    "    t['clf']['test']['mel'] = join(t['clf']['test']['base'], 'melanoma')\n",
    "\n",
    "    return t\n",
    "\n",
    "t_dir = define_transform_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dirs for transforemed data\n",
    "import os\n",
    "def mkdir(dir_path): \n",
    "    if not os.path.exists(dir_path): os.makedirs(dir_path)\n",
    "\n",
    "mkdir(t_dir['seg']['val']['img'])\n",
    "mkdir(t_dir['seg']['val']['msk'])\n",
    "mkdir(t_dir['seg']['train']['img'])\n",
    "mkdir(t_dir['seg']['train']['msk'])\n",
    "mkdir(t_dir['seg']['test']['img'])\n",
    "mkdir(t_dir['seg']['test']['msk'])\n",
    "\n",
    "mkdir(t_dir['clf']['val']['nev'])\n",
    "mkdir(t_dir['clf']['val']['mel'])\n",
    "mkdir(t_dir['clf']['train']['nev'])\n",
    "mkdir(t_dir['clf']['train']['mel'])\n",
    "mkdir(t_dir['clf']['test']['nev'])\n",
    "mkdir(t_dir['clf']['test']['mel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ISICDataset_seg\n",
    "%run '3.1.Dataset.ipynb'\n",
    "\n",
    "# Import custum transformers\n",
    "%run '3.3.Transformers.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation data transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print('Segmentation preprocessing...')\n",
    "\n",
    "# train_transforms = [\n",
    "#     Resize(t_config['img_size']),\n",
    "#     RandAffine(),\n",
    "#     RandHFlip(),\n",
    "#     AdjBrightness(),\n",
    "#     AdjContrast(),\n",
    "#     AdjSaturation(0,1.5),\n",
    "#     AdjHue(-0.1, 0.1),\n",
    "#     ToTensor()\n",
    "# ]\n",
    "\n",
    "# test_transforms = [\n",
    "#     Resize(t_config['img_size']),\n",
    "#     RandAffine(),\n",
    "#     RandHFlip(),\n",
    "#     ToTensor()\n",
    "# ]\n",
    "\n",
    "# validation_transforms = [\n",
    "#     Resize(t_config['img_size']),\n",
    "#     RandAffine(),\n",
    "#     RandHFlip(),\n",
    "#     ToTensor()\n",
    "# ]\n",
    "\n",
    "# validation_dataset = ISICDataset_seg(seg_validation_dir, validation_transforms)\n",
    "# train_dataset = ISICDataset_seg(seg_train_dir, train_transforms)\n",
    "# test_dataset = ISICDataset_seg(seg_test_dir, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG poligon\n",
    "# if not IS_MAIN:\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from torchvision.transforms import ToPILImage\n",
    "    \n",
    "#     sample = train_dataset[14]\n",
    "#     untransform = ToPILImage()\n",
    "\n",
    "#     img = untransform(sample['image'])\n",
    "#     msk = untransform(sample['mask'])\n",
    "\n",
    "#     fig, ax = plt.subplots(1,2)\n",
    "#     ax[0].imshow(img)\n",
    "#     ax[1].imshow(msk, 'gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes for transform generators\n",
    "def get_indexes(dataset, preferred_size):\n",
    "    # make list of indexes\n",
    "    indexes = []\n",
    "    img_count = 0\n",
    "    i = 0\n",
    "\n",
    "    while img_count < preferred_size:\n",
    "        indexes += [[i, img_count]]\n",
    "        img_count += 1\n",
    "        i += 1\n",
    "        if i >= len(dataset):\n",
    "            i = 0 \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. TRAIN\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torchvision.transforms import ToPILImage\n",
    "untransform = ToPILImage()\n",
    "\n",
    "# paralell function\n",
    "# !!! Must define ransforms inside each thread\n",
    "def transform_train(index):\n",
    "    train_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        AdjBrightness(),\n",
    "        AdjContrast(),\n",
    "#         AdjSaturation(0,1.5),\n",
    "        AdjHue(-0.1, 0.1),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    train_dataset = ISICDataset_seg(seg_train_dir, train_transforms)\n",
    "    sample = train_dataset[index[0]]\n",
    "    \n",
    "    img = untransform(sample['image'])\n",
    "    msk = untransform(sample['mask'])\n",
    "    \n",
    "    img.save(t_dir['seg']['train']['img']+'/'+str(index[1])+'.png')\n",
    "    msk.save(t_dir['seg']['train']['msk']+'/'+str(index[1])+'.png')    \n",
    "\n",
    "train_dataset = ISICDataset_seg(seg_train_dir)\n",
    "indexes = get_indexes(train_dataset, t_config['seg']['train_size'])\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_train, indexes)\n",
    "print('train image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. TEST\n",
    "def transform_test(index):\n",
    "    test_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    test_dataset = ISICDataset_seg(seg_test_dir, test_transforms)\n",
    "    sample = test_dataset[index[0]]\n",
    "    \n",
    "    img = untransform(sample['image'])\n",
    "    msk = untransform(sample['mask'])\n",
    "    \n",
    "    img.save(t_dir['seg']['test']['img']+'/'+str(index[1])+'.png')\n",
    "    msk.save(t_dir['seg']['test']['msk']+'/'+str(index[1])+'.png')  \n",
    "    \n",
    "test_dataset = ISICDataset_seg(seg_test_dir)\n",
    "indexes = get_indexes(test_dataset, t_config['seg']['test_size'])\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_test, indexes)\n",
    "print('test image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. VALIDATION\n",
    "\n",
    "def transform_validation(index):\n",
    "    validation_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    validation_dataset = ISICDataset_seg(seg_validation_dir, validation_transforms)\n",
    "    sample = validation_dataset[index[0]]\n",
    "    \n",
    "    img = untransform(sample['image'])\n",
    "    msk = untransform(sample['mask'])\n",
    "    \n",
    "    img.save(t_dir['seg']['val']['img']+'/'+str(index[1])+'.png')\n",
    "    msk.save(t_dir['seg']['val']['msk']+'/'+str(index[1])+'.png')  \n",
    "    \n",
    "validation_dataset = ISICDataset_seg(seg_validation_dir)\n",
    "indexes = get_indexes(validation_dataset, t_config['seg']['val_size'])\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_validation, indexes)\n",
    "print('validation image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images generated     : 500/500\n",
      "Train masks generated      : 500/500\n",
      "Test images generated      : 100/100\n",
      "Test masks generated       : 100/100\n",
      "Validation images generated: 200/200\n",
      "Validation masks generated : 200/200\n"
     ]
    }
   ],
   "source": [
    "print('Train images generated     : {}/{}'.format(len(os.listdir(t_dir['seg']['train']['img'])),t_config['seg']['train_size'])) \n",
    "print('Train masks generated      : {}/{}'.format(len(os.listdir(t_dir['seg']['train']['msk'])),t_config['seg']['train_size']))\n",
    "print('Test images generated      : {}/{}'.format(len(os.listdir(t_dir['seg']['test']['img'])),t_config['seg']['test_size']))\n",
    "print('Test masks generated       : {}/{}'.format(len(os.listdir(t_dir['seg']['test']['msk'])),t_config['seg']['test_size']))\n",
    "print('Validation images generated: {}/{}'.format(len(os.listdir(t_dir['seg']['val']['img'])),t_config['seg']['val_size']))\n",
    "print('Validation masks generated : {}/{}'.format(len(os.listdir(t_dir['seg']['val']['msk'])),t_config['seg']['val_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print('Classification preprocessing...')\n",
    "\n",
    "# validation_dataset = ISICDataset_clf(clf_validation_dir, validation_transforms, shuffle=False)\n",
    "# train_dataset = ISICDataset_clf(clf_train_dir, train_transforms, shuffle=False)\n",
    "# test_dataset = ISICDataset_clf(clf_test_dir, test_transforms, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG poligon\n",
    "# if not IS_MAIN:\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from torchvision.transforms import ToPILImage\n",
    "    \n",
    "#     sample = train_dataset[21]\n",
    "#     untransform = ToPILImage()\n",
    "\n",
    "#     img = untransform(sample['image'])\n",
    "\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filecount(dataset): \n",
    "    melanoma_files = (dataset.labels == 1).sum().item()\n",
    "    nevus_files = len(dataset) - melanoma_files\n",
    "    return melanoma_files, nevus_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes for transform generators\n",
    "def get_indexes_clf(dataset, real_size, preferred_size, start_idx=0):\n",
    "    # make list of indexes\n",
    "    indexes = []\n",
    "    img_count = 0\n",
    "    i = start_idx\n",
    "\n",
    "    while img_count < preferred_size:\n",
    "        indexes += [[i, img_count]]\n",
    "        img_count += 1\n",
    "        i += 1\n",
    "        \n",
    "        if i > real_size:\n",
    "            i = 0 \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. TRAIN\n",
    "\n",
    "def transform_train_mel(index):\n",
    "    train_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        AdjBrightness(),\n",
    "        AdjContrast(),\n",
    "#         AdjSaturation(0,1.5),\n",
    "        AdjHue(-0.1, 0.1),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    train_dataset = ISICDataset_clf(clf_train_dir, train_transforms, shuffle=False)\n",
    "    sample = train_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['train']['mel']+'/'+str(index[1])+'.png')\n",
    "\n",
    "def transform_train_nev(index):\n",
    "    train_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        AdjBrightness(),\n",
    "        AdjContrast(),\n",
    "#         AdjSaturation(0,1.5),\n",
    "        AdjHue(-0.1, 0.1),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    train_dataset = ISICDataset_clf(clf_train_dir, train_transforms, shuffle=False)\n",
    "    sample = train_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['train']['nev']+'/'+str(index[1])+'.png')\n",
    "\n",
    "    \n",
    "train_dataset = ISICDataset_clf(clf_train_dir, shuffle=False)\n",
    "mel_count, nev_count = get_filecount(train_dataset)\n",
    "mel_indexes = get_indexes_clf(train_dataset, mel_count,\n",
    "                              t_config['clf']['train_size']['mel'])\n",
    "nev_indexes = get_indexes_clf(train_dataset, nev_count,\n",
    "                              t_config['clf']['train_size']['nev'], start_idx=mel_count)\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_train_mel, mel_indexes)\n",
    "    \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_train_nev, nev_indexes)\n",
    "    \n",
    "print('train image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. VALIDATION\n",
    "\n",
    "def transform_val_mel(index):\n",
    "    validation_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    validation_dataset = ISICDataset_clf(clf_validation_dir, validation_transforms, shuffle=False)\n",
    "    sample = validation_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['val']['mel']+'/'+str(index[1])+'.png')\n",
    "\n",
    "def transform_val_nev(index):\n",
    "    validation_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    validation_dataset = ISICDataset_clf(clf_validation_dir, validation_transforms, shuffle=False)\n",
    "    sample = validation_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['val']['nev']+'/'+str(index[1])+'.png')\n",
    "\n",
    "validation_dataset = ISICDataset_clf(clf_validation_dir, shuffle=False)\n",
    "mel_count, nev_count = get_filecount(validation_dataset)\n",
    "mel_indexes = get_indexes_clf(validation_dataset, mel_count,\n",
    "                              t_config['clf']['val_size']['mel'])\n",
    "nev_indexes = get_indexes_clf(validation_dataset, nev_count,\n",
    "                              t_config['clf']['val_size']['nev'], start_idx=mel_count)\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_val_mel, mel_indexes)\n",
    "    \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_val_nev, nev_indexes)\n",
    "    \n",
    "print('validation image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image transform done.\n"
     ]
    }
   ],
   "source": [
    "# Paralell image transformer. TEST\n",
    "\n",
    "def transform_test_mel(index):\n",
    "    test_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    test_dataset = ISICDataset_clf(clf_test_dir, test_transforms, shuffle=False)\n",
    "    sample = test_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['test']['mel']+'/'+str(index[1])+'.png')\n",
    "\n",
    "def transform_test_nev(index):\n",
    "    test_transforms = [\n",
    "        Resize(t_config['img_size']),\n",
    "        RandAffine(),\n",
    "        RandHFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    test_dataset = ISICDataset_clf(clf_test_dir, test_transforms, shuffle=False)\n",
    "    sample = test_dataset[index[0]]\n",
    "    img = untransform(sample['image'])\n",
    "    \n",
    "    img.save(t_dir['clf']['test']['nev']+'/'+str(index[1])+'.png')\n",
    "\n",
    "test_dataset = ISICDataset_clf(clf_test_dir, shuffle=False)    \n",
    "mel_count, nev_count = get_filecount(test_dataset)\n",
    "mel_indexes = get_indexes_clf(test_dataset, mel_count,\n",
    "                              t_config['clf']['test_size']['mel'])\n",
    "nev_indexes = get_indexes_clf(test_dataset, nev_count,\n",
    "                              t_config['clf']['test_size']['nev'], start_idx=mel_count)\n",
    "\n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_test_mel, mel_indexes)\n",
    "    \n",
    "with ThreadPoolExecutor() as e: \n",
    "    e.map(transform_test_nev, nev_indexes)\n",
    "    \n",
    "print('test image transform done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images generated     : 250/250\n",
      "Train masks generated      : 250/250\n",
      "Test images generated      : 50/50\n",
      "Test masks generated       : 50/50\n",
      "Validation images generated: 100/100\n",
      "Validation masks generated : 100/100\n"
     ]
    }
   ],
   "source": [
    "print('Train images generated     : {}/{}'.format(len(os.listdir(t_dir['clf']['train']['nev'])),t_config['clf']['train_size']['nev'])) \n",
    "print('Train masks generated      : {}/{}'.format(len(os.listdir(t_dir['clf']['train']['mel'])),t_config['clf']['train_size']['mel']))\n",
    "print('Test images generated      : {}/{}'.format(len(os.listdir(t_dir['clf']['test']['nev'])),t_config['clf']['test_size']['nev']))\n",
    "print('Test masks generated       : {}/{}'.format(len(os.listdir(t_dir['clf']['test']['mel'])),t_config['clf']['test_size']['mel']))\n",
    "print('Validation images generated: {}/{}'.format(len(os.listdir(t_dir['clf']['val']['nev'])),t_config['clf']['val_size']['nev']))\n",
    "print('Validation masks generated : {}/{}'.format(len(os.listdir(t_dir['clf']['val']['mel'])),t_config['clf']['val_size']['mel']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
